{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs: 30\n",
      "Total Chunks Created: 37768\n",
      "['5 1.5.2 Central Processing Unit CPU is the major component which interprets and executes software instructions. It also control the operation of all other components such as memory, input and output units. It accepts binary data as input, process the data according to the instructions and provide the result as output. The CPU has three components which are Control unit, Arithmetic and logic unit (ALU) and Memory unit.', '1.5.2.1 Arithmetic and Logic Unit The ALU is a part of the CPU where various computing functions are performed on data. The ALU performs arithmetic operations such as addition, subtraction, multiplication, division and logical operations. The result of an operation is stored in internal memory of CPU. The logical operations of ALU promote the decision-making ability of a computer. 1.5.2.2 Control Unit The control unit controls the flow of data between the CPU, memory and I/O devices.', 'It also controls the entire operation of a computer. 1.5.3. Output Unit An Output Unit is any hardware component that conveys information to users in an understandable form. Example: Monitor, Printer etc. 1.5.4. Memory Unit The Memory Unit is of two types which are primary memory and secondary memory. The primary memory is used to temporarily store the programs and data when the instructions are ready to execute. The secondary memory is used to store the data permanently.']\n",
      "âœ… Extracted and saved 37768 chunks.\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    total_pages = len(doc)\n",
    "    \n",
    "    if total_pages <= 20:\n",
    "        print(f\"Skipping {pdf_path} as it has less than 20 pages.\")\n",
    "        return \"\"\n",
    "    \n",
    "    for page_num in range(10, total_pages - 10):\n",
    "        text += doc[page_num].get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_and_chunk_text(text, chunk_size=500):\n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  \n",
    "    chunks, chunk = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \" \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "pdf_folder = \"../public/resources\"\n",
    "pdf_files = os.listdir(pdf_folder)\n",
    "pdf_files = [pdf for pdf in pdf_files if pdf.endswith(\".pdf\")][:30]\n",
    "print(f\"Total PDFs: {len(pdf_files)}\")\n",
    "\n",
    "all_chunks = []\n",
    "pdf_texts = []\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    if raw_text:\n",
    "        chunks = clean_and_chunk_text(raw_text)\n",
    "        all_chunks.extend(chunks)\n",
    "        pdf_texts.append({\"title\": os.path.splitext(pdf)[0], \"filename\": pdf})\n",
    "\n",
    "print(f\"Total Chunks Created: {len(all_chunks)}\")\n",
    "print(all_chunks[:3])\n",
    "\n",
    "with open(\"all_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(f\"âœ… Extracted and saved {len(all_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulmarban/Documents/Coding/kalvikaram/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-08 18:07:00,299 - INFO - Loading Sentence Transformer model on MPS...\n",
      "2025-03-08 18:07:00,301 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-03-08 18:07:06,489 - INFO - Encoding chunks with optimized batch size (256)...\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [08:27<00:00,  3.43s/it]\n",
      "2025-03-08 18:15:34,469 - INFO - Encoding complete! Total vectors: 37768, Dimension: 384\n",
      "2025-03-08 18:15:34,472 - INFO - Initializing FAISS IVF index with 1024 clusters...\n",
      "2025-03-08 18:15:34,480 - INFO - Training FAISS index...\n",
      "WARNING clustering 37768 points to 1024 centroids: please provide at least 39936 training points\n",
      "2025-03-08 18:15:35,384 - INFO - Adding embeddings to FAISS index...\n",
      "2025-03-08 18:15:35,543 - INFO - FAISS index stored successfully at 'faiss_index.bin'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors: 37768\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "logging.info(\"Loading Sentence Transformer model on MPS...\")\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device=\"mps\")\n",
    "\n",
    "logging.info(\"Encoding chunks with optimized batch size (256)...\")\n",
    "embeddings = model.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "embeddings = np.ascontiguousarray(embeddings.astype(np.float32))\n",
    "logging.info(f\"Encoding complete! Total vectors: {len(embeddings)}, Dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "n_clusters = 1024 \n",
    "\n",
    "logging.info(f\"Initializing FAISS IVF index with {n_clusters} clusters...\")\n",
    "quantizer = faiss.IndexFlatL2(dimension) \n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters, faiss.METRIC_L2)\n",
    "\n",
    "\n",
    "logging.info(\"Training FAISS index...\")\n",
    "index.train(embeddings)\n",
    "\n",
    "logging.info(\"Adding embeddings to FAISS index...\")\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "logging.info(\"FAISS index stored successfully at 'faiss_index.bin'.\")\n",
    "\n",
    "print(f\"Total vectors: {len(embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 18:15:36,203 - INFO - Use pytorch device_name: mps\n",
      "2025-03-08 18:15:36,203 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "with open(\"all_chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
    "generative_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def search_faiss(query, top_k=5, distance_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Retrieve top matching text chunks from FAISS index with a distance filter.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = [\n",
    "        all_chunks[idx] \n",
    "        for idx, dist in zip(indices[0], distances[0]) \n",
    "        if idx < len(all_chunks) and dist < distance_threshold\n",
    "    ]\n",
    "    return results if results else [\"No relevant information found.\"]\n",
    "\n",
    "def generate_answer(query):\n",
    "    retrieved_chunks = search_faiss(query)\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"\n",
    "You are an tamil AI assistant that uses textbook content to answer student queries with tamil books.\n",
    "Given the query and context below, generate a relevant answer based on the context, if the answer is not in the context, you can generate the answer based on the context but dont mention that its not in context and dont say sorry.\n",
    "Make it simple and easy to understand and learn, if its a english name like a theory name then maintain english but if there is a tamil meaning for it then explain in tamil.\n",
    "Dont literally translate to tamil, say it in tamil meaning instead of literal english to tamil. You can give summarization about it and talk about stuff thats around the topic that was asked.\n",
    "Im studying in tamil medium so please make it easy to understand and learn and also if it has a complex english word then mention the english part in side with paranthesis. Use proper tamil words and translation\n",
    "\n",
    "If i ask something thats wayyy off topic then just say that has nothing to do with the topic.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "    response = generative_model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Query: What is gibbs free energy\n",
      "ðŸ¤– AI Answer: à®‰à®™à¯à®• à®•à¯‡à®³à¯à®µà®¿ Gibbs Free Energy à®ªà®¤à¯à®¤à®¿à®©à®¤à¯. à®‡à®¤à¯ à®µà¯‡à®¤à®¿à®¯à®¿à®¯à®²à¯à®² (chemistry) à®°à¯Šà®®à¯à®ª à®®à¯à®•à¯à®•à®¿à®¯à®®à®¾à®© à®’à®°à¯ à®•à®°à¯à®¤à¯à®¤à¯. à®’à®°à¯ à®µà¯‡à®¤à®¿à®µà®¿à®©à¯ˆ (chemical reaction) à®¤à®¾à®©à®¾ à®¨à®Ÿà®•à¯à®•à¯à®®à®¾, à®¨à®Ÿà®•à¯à®•à®¾à®¤à®¾à®©à¯à®©à¯ à®•à®£à¯à®Ÿà¯à®ªà®¿à®Ÿà®¿à®•à¯à®• à®‡à®¤à¯ à®‰à®¤à®µà¯à®®à¯.\n",
      "\n",
      "Gibbs Free Energy-à®™à¯à®•à®¿à®±à®¤à¯ à®’à®°à¯ à®šà®¿à®¸à¯à®Ÿà®¤à¯à®¤à¯‹à®Ÿ (system) à®Žà®©à®°à¯à®œà®¿à®¯ à®•à¯à®±à®¿à®•à¯à®•à¯à®¤à¯. à®’à®°à¯ à®šà®¿à®¸à¯à®Ÿà®®à¯ à®Žà®µà¯à®µà®³à®µà¯ à®µà¯‡à®²à¯ˆ à®šà¯†à®¯à¯à®¯ à®®à¯à®Ÿà®¿à®¯à¯à®®à¯ à®…à®ªà¯à®ªà®Ÿà®¿à®™à¯à®•à¯à®±à®¤à¯ˆà®¯à¯à®®à¯ à®‡à®¤à¯ à®šà¯Šà®²à¯à®²à¯à®®à¯. à®‡à®¤à¯ˆ G à®…à®ªà¯à®ªà®Ÿà®¿à®™à¯à®•à¯à®± à®Žà®´à¯à®¤à¯à®¤à®¾à®² à®•à¯à®±à®¿à®ªà¯à®ªà®¾à®™à¯à®•.\n",
      "\n",
      "à®’à®°à¯ à®µà¯‡à®¤à®¿à®µà®¿à®©à¯ˆ à®¨à®Ÿà®•à¯à®•à¯à®®à¯à®ªà¯‹à®¤à¯, Gibbs Free Energy à®•à¯à®±à¯ˆà®¯à¯à®®à¯. à®…à®¤à®¾à®µà®¤à¯, G-à®¯à¯‹à®Ÿ à®®à®¤à®¿à®ªà¯à®ªà¯ à®•à¯à®±à¯ˆà®žà¯à®šà®¾, à®…à®¨à¯à®¤ à®µà®¿à®©à¯ˆ à®¤à®¾à®©à®¾ à®¨à®Ÿà®•à¯à®•à¯à®®à¯. G-à®¯à¯‹à®Ÿ à®®à®¤à®¿à®ªà¯à®ªà¯ à®…à®¤à®¿à®•à®®à®¾ à®‡à®°à¯à®¨à¯à®¤à®¾, à®…à®¨à¯à®¤ à®µà®¿à®©à¯ˆ à®¤à®¾à®©à®¾ à®¨à®Ÿà®•à¯à®•à®¾à®¤à¯. à®¨à®¾à®® à®Žà®©à®°à¯à®œà®¿ à®•à¯Šà®Ÿà¯à®¤à¯à®¤à®¾ à®¤à®¾à®©à¯ à®¨à®Ÿà®•à¯à®•à¯à®®à¯.\n",
      "\n",
      "à®šà¯à®°à¯à®•à¯à®•à®®à®¾ à®šà¯Šà®²à¯à®²à®£à¯à®®à¯à®©à®¾, Gibbs Free Energy à®’à®°à¯ à®µà¯‡à®¤à®¿à®µà®¿à®©à¯ˆ à®¤à®¾à®©à®¾ à®¨à®Ÿà®•à¯à®•à¯à®®à®¾, à®¨à®Ÿà®•à¯à®•à®¾à®¤à®¾à®©à¯à®©à¯ à®šà¯Šà®²à¯à®²à¯à®± à®’à®°à¯ à®•à®°à¯à®µà®¿ à®®à®¾à®¤à®¿à®°à®¿.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Query: Difference between mass and weight\n",
      "ðŸ¤– AI Answer: à®¨à®¿à®±à¯ˆ (Mass) à®®à®±à¯à®±à¯à®®à¯ à®Žà®Ÿà¯ˆ (Weight) à®†à®•à®¿à®¯à®µà®±à¯à®±à¯à®•à¯à®•à¯ à®‡à®Ÿà¯ˆà®¯à¯‡à®¯à®¾à®© à®µà¯‡à®±à¯à®ªà®¾à®Ÿà¯ à®Žà®©à¯à®©à®µà¯†à®©à¯à®±à¯ à®ªà®¾à®°à¯à®ªà¯à®ªà¯‹à®®à¯.\n",
      "\n",
      "**à®¨à®¿à®±à¯ˆ (Mass):**\n",
      "\n",
      "*   à®¨à®¿à®±à¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®ªà®°à¯à®ªà¯à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®…à®³à®µà¯ˆà®•à¯ à®•à¯à®±à®¿à®•à¯à®•à®¿à®±à®¤à¯. à®…à®¤à®¾à®µà®¤à¯, à®’à®°à¯ à®ªà¯Šà®°à¯à®³à¯ à®Žà®µà¯à®µà®³à®µà¯ \"stuff\" à®†à®²à¯ à®†à®©à®¤à¯ à®Žà®©à¯à®ªà®¤à¯ˆà®ªà¯ à®ªà®±à¯à®±à®¿à®¯à®¤à¯.\n",
      "*   à®‡à®¤à¯ à®’à®°à¯ à®®à®¾à®±à®¾à®¤ à®®à®¤à®¿à®ªà¯à®ªà¯. à®…à®¤à®¾à®µà®¤à¯, à®¨à¯€à®™à¯à®•à®³à¯ à®ªà¯‚à®®à®¿à®¯à®¿à®²à¯ à®‡à®°à¯à®¨à¯à®¤à®¾à®²à¯à®®à¯ à®šà®°à®¿, à®šà®¨à¯à®¤à®¿à®°à®©à®¿à®²à¯ à®‡à®°à¯à®¨à¯à®¤à®¾à®²à¯à®®à¯ à®šà®°à®¿, à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®¨à®¿à®±à¯ˆ à®Žà®ªà¯à®ªà¯‹à®¤à¯à®®à¯ à®’à®°à¯‡ à®®à®¾à®¤à®¿à®°à®¿à®¯à®¾à®• à®‡à®°à¯à®•à¯à®•à¯à®®à¯.\n",
      "*   à®¨à®¿à®±à¯ˆà®¯à¯ˆ à®•à®¿à®²à¯‹à®•à®¿à®°à®¾à®®à®¿à®²à¯ (kilogram - kg) à®…à®³à®µà®¿à®Ÿà¯à®•à®¿à®±à¯‹à®®à¯.\n",
      "\n",
      "**à®Žà®Ÿà¯ˆ (Weight):**\n",
      "\n",
      "*   à®Žà®Ÿà¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®®à¯€à®¤à¯ à®ªà¯à®µà®¿à®¯à¯€à®°à¯à®ªà¯à®ªà¯ à®µà®¿à®šà¯ˆ (gravitational force) à®Žà®µà¯à®µà®³à®µà¯ à®šà¯†à®¯à®²à¯à®ªà®Ÿà¯à®•à®¿à®±à®¤à¯ à®Žà®©à¯à®ªà®¤à¯ˆà®•à¯ à®•à¯à®±à®¿à®•à¯à®•à®¿à®±à®¤à¯.\n",
      "*   à®Žà®Ÿà¯ˆ à®®à®¾à®±à¯à®®à¯ à®®à®¤à®¿à®ªà¯à®ªà¯. à®à®©à¯†à®©à®¿à®²à¯, à®ªà¯à®µà®¿à®¯à¯€à®°à¯à®ªà¯à®ªà¯ à®µà®¿à®šà¯ˆ à®‡à®Ÿà®¤à¯à®¤à®¿à®±à¯à®•à¯ à®‡à®Ÿà®®à¯ à®®à®¾à®±à¯à®ªà®Ÿà¯à®®à¯. à®‰à®¤à®¾à®°à®£à®®à®¾à®•, à®ªà¯‚à®®à®¿à®¯à®¿à®²à¯ à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®Žà®Ÿà¯ˆ à®…à®¤à®¿à®•à®®à®¾à®• à®‡à®°à¯à®•à¯à®•à¯à®®à¯, à®†à®©à®¾à®²à¯ à®šà®¨à¯à®¤à®¿à®°à®©à®¿à®²à¯ à®•à¯à®±à¯ˆà®µà®¾à®• à®‡à®°à¯à®•à¯à®•à¯à®®à¯.\n",
      "*   à®Žà®Ÿà¯ˆà®¯à¯ˆ à®¨à®¿à®¯à¯‚à®Ÿà¯à®Ÿà®©à®¿à®²à¯ (Newton - N) à®…à®³à®µà®¿à®Ÿà¯à®•à®¿à®±à¯‹à®®à¯.\n",
      "\n",
      "**à®šà¯à®°à¯à®•à¯à®•à®®à®¾à®•:**\n",
      "\n",
      "à®¨à®¿à®±à¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®ªà®°à¯à®ªà¯à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®…à®³à®µà¯, à®Žà®Ÿà¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®ªà¯à®µà®¿à®¯à¯€à®°à¯à®ªà¯à®ªà¯ à®µà®¿à®šà¯ˆà®¯à®¾à®²à¯ à®…à®¨à¯à®¤à®ªà¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®®à¯€à®¤à¯ à®à®±à¯à®ªà®Ÿà¯à®®à¯ à®µà®¿à®šà¯ˆ.\n",
      "\n",
      "à®Žà®³à®¿à®®à¯ˆà®¯à®¾à®•à®šà¯ à®šà¯Šà®©à¯à®©à®¾à®²à¯, à®¨à®¿à®±à¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®’à®°à¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ \"à®‰à®£à¯à®®à¯ˆ\" à®…à®³à®µà¯, à®Žà®Ÿà¯ˆ à®Žà®©à¯à®ªà®¤à¯ à®…à®¨à¯à®¤à®ªà¯ à®ªà¯Šà®°à¯à®³à®¿à®©à¯ à®®à¯€à®¤à¯ à®ªà¯à®µà®¿à®¯à¯€à®°à¯à®ªà¯à®ªà¯ à®Žà®µà¯à®µà®³à®µà¯ \"à®‡à®´à¯à®•à¯à®•à®¿à®±à®¤à¯\" à®Žà®©à¯à®ªà®¤à¯ˆà®•à¯ à®•à¯à®±à®¿à®•à¯à®•à®¿à®±à®¤à¯.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Query: What is the first law of thermodynamics\n",
      "ðŸ¤– AI Answer: à®µà¯†à®ªà¯à®ª à®‡à®¯à®•à¯à®•à®µà®¿à®¯à®²à®¿à®©à¯ à®®à¯à®¤à®²à¯ à®µà®¿à®¤à®¿ (First law of thermodynamics) à®Žà®©à¯à®ªà®¤à¯ à®†à®±à¯à®±à®²à¯ à®…à®´à®¿à®µà®¿à®©à¯à®®à¯ˆ à®µà®¿à®¤à®¿à®¯à®¿à®©à¯ˆ (law of conservation of energy) à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆà®¯à®¾à®•à®•à¯ à®•à¯Šà®£à¯à®Ÿà®¤à¯. à®‡à®¨à¯à®¤ à®µà®¿à®¤à®¿à®¯à®¿à®©à¯à®ªà®Ÿà®¿, à®’à®°à¯ à®¤à®©à®¿à®®à¯ˆà®ªà¯à®ªà®Ÿà¯à®¤à¯à®¤à®ªà¯à®ªà®Ÿà¯à®Ÿ à®…à®®à¯ˆà®ªà¯à®ªà®¿à®²à¯ (isolated system) à®†à®±à¯à®±à®²à¯ˆ à®‰à®°à¯à®µà®¾à®•à¯à®•à®µà¯‹ à®…à®´à®¿à®•à¯à®•à®µà¯‹ à®®à¯à®Ÿà®¿à®¯à®¾à®¤à¯, à®†à®©à®¾à®²à¯ à®’à®°à¯ à®µà®Ÿà®¿à®µà®¤à¯à®¤à®¿à®²à®¿à®°à¯à®¨à¯à®¤à¯ à®®à®±à¯à®±à¯Šà®°à¯ à®µà®Ÿà®¿à®µà®¤à¯à®¤à®¿à®±à¯à®•à¯ à®®à®¾à®±à¯à®± à®®à¯à®Ÿà®¿à®¯à¯à®®à¯.\n",
      "\n",
      "à®Žà®³à®¿à®®à¯ˆà®¯à®¾à®• à®šà¯Šà®²à¯à®²à®©à¯à®®à¯à®©à®¾, à®’à®°à¯ à®®à¯‚à®Ÿà®¿à®¯ à®…à®®à¯ˆà®ªà¯à®ªà¯à®² (closed system) à®‡à®°à¯à®•à¯à®•à®¿à®± à®®à¯Šà®¤à¯à®¤ à®†à®±à¯à®±à®²à¯ à®Žà®ªà¯à®ªà®µà¯à®®à¯ à®®à®¾à®±à®¾à®® à®‡à®°à¯à®•à¯à®•à¯à®®à¯. à®†à®±à¯à®±à®²à¯ˆ à®’à®°à¯ à®‡à®Ÿà®¤à¯à®¤à¯à®² à®‡à®°à¯à®¨à¯à®¤à¯ à®‡à®©à¯à®©à¯Šà®°à¯ à®‡à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®®à®¾à®±à¯à®±à®²à®¾à®®à¯, à®‡à®²à¯à®²à®©à®¾ à®’à®°à¯ à®µà®•à¯ˆà®¯à®¾à®© à®†à®±à¯à®±à®²à¯ˆ à®‡à®©à¯à®©à¯Šà®°à¯ à®µà®•à¯ˆà®¯à®¾à®© à®†à®±à¯à®±à®²à®¾ à®®à®¾à®±à¯à®±à®²à®¾à®®à¯, à®†à®©à®¾ à®…à®¨à¯à®¤ à®®à¯Šà®¤à¯à®¤ à®†à®±à¯à®±à®²à¯‹à®Ÿ à®…à®³à®µà¯ à®®à®¾à®±à®µà¯‡ à®®à®¾à®±à®¾à®¤à¯.\n",
      "\n",
      "à®‰à®¤à®¾à®°à®£à®¤à¯à®¤à¯à®•à¯à®•à¯, à®’à®°à¯ à®µà®£à¯à®Ÿà®¿à®•à¯à®•à¯ à®ªà¯†à®Ÿà¯à®°à¯‹à®²à¯ (petrol) à®ªà¯‹à®Ÿà¯à®®à¯à®ªà¯‹à®¤à¯, à®ªà¯†à®Ÿà¯à®°à¯‹à®²à¯à®² à®‡à®°à¯à®•à¯à®•à®¿à®± à®‡à®°à®šà®¾à®¯à®© à®†à®±à¯à®±à®²à¯ (chemical energy) à®µà®£à¯à®Ÿà®¿ à®“à®Ÿà¯à®®à¯à®ªà¯‹à®¤à¯ à®‡à®¯à®•à¯à®• à®†à®±à¯à®±à®²à®¾ (kinetic energy) à®®à®¾à®±à¯à®¤à¯. à®†à®©à®¾, à®…à®¨à¯à®¤ à®®à¯Šà®¤à¯à®¤ à®†à®±à¯à®±à®²à¯‹à®Ÿ à®…à®³à®µà¯ à®®à®¾à®±à®¾à®¤à¯. à®‡à®¤à¯à®¤à®¾à®©à¯ à®µà¯†à®ªà¯à®ª à®‡à®¯à®•à¯à®•à®µà®¿à®¯à®²à®¿à®©à¯ à®®à¯à®¤à®²à¯ à®µà®¿à®¤à®¿ à®šà¯Šà®²à¯à®²à¯à®¤à¯.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is gibbs free energy\",\n",
    "    \"Difference between mass and weight\",\n",
    "    \"What is the first law of thermodynamics\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = generate_answer(query)\n",
    "    print(f\"\\nðŸ” Query: {query}\")\n",
    "    print(f\"ðŸ¤– AI Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "dimension = 384\n",
    "\n",
    "num_vectors = index.ntotal\n",
    "if num_vectors < 5:\n",
    "    exit()\n",
    "\n",
    "if not hasattr(index, 'direct_map') or index.direct_map.type == 0:\n",
    "    exit()\n",
    "\n",
    "embeddings = np.zeros((num_vectors, dimension), dtype=np.float32)\n",
    "for i in range(num_vectors):\n",
    "    index.reconstruct(i, embeddings[i])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "perplexity_value = min(30, num_vectors - 1)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "plt.title(\"t-SNE Visualization of Text Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAISS Retrieved Chunks:\n",
      "- UNIT-1(XI-Physics_Vol-1).indd 16 UNIT-1(XI-Physics_Vol-1).indd 16 05-01-2022 22:11:42 05-01-2022 22:11:42 www.tntextbooks.in Unit 1 Nature of Physical World and Measurement 17 1.5.2 Measurement of mas...\n",
      "- The masses of objects which we shall study in this course vary over a wide range. These may vary from a tiny mass of electron (9.11Ã—10âˆ’31kg) to the huge mass of the known universe (=1055 kg). The orde...\n",
      "- Gram equivalent mass is the mass of an element (compound/ ion) that combines or displaces 1.008 g hydrogen, 8 g oxygen or 35.5 g chlorine. Elemental analysis of a compound gives the mass percentage of...\n",
      "- Solution: Mass m1 = 60 kg Mass m2 = 30 kg V cms V cm s 1 1 2 1 40 30 Solution: v m m m m u m m m u v m m m m u 1 1 2 1 2 1 2 1 2 2 2 2 1 1 2 2 2 1 2 1 2m1 m m u Substituting the values, we get, v1 60 ...\n",
      "- = 4 34 0 31 14 . . = âˆ´ Empirical formula = Na2 S H20 O14 n molar mass calculated empirical formula mass = = = 322 322 1 Na S H O 2 20 14 2 23 1 32 20 1 14 16 46 32 20 224 322 = Ã— + Ã— + Ã— + = + + + = ï£®...\n",
      "\n",
      "BM25 Retrieved Chunks:\n",
      "- The spring constant k =1 N m-1 and assume that the surface is frictionless. (a) When the mass crosses the equilibrium position, what is the speed of the mass? (b) What is the force that acts on the ob...\n",
      "- (a) 200 atoms of x + 200 atoms of y + 50 molecules of z2 (b) 1mol of x + 1 mol of y+3 mol of z2 (c) 50 atoms of x + 25 atoms of y+50 molecules of z2 d) 2.5 mol of x +5 mol of y+5 mol of z2 36) Mass of...\n",
      "- A uniform rod of mass m and length l makes a constant angle Î¸ with an axis of rotation which passes through one end of the rod. Find the moment of inertia about this axis. Ans: 1 3 2 2 Mï¬sin Î¸ 5. Two ...\n",
      "- Folding of fatty aids TN_GOVT_XI_Micro_Biology_CH07.indd 109 16-02-2019 9.59.33 AM www.tntextbooks.in 110 Answer the following 1. What is Glycocalyx? 2. What is a capsule? What are its functions? 3. W...\n",
      "- 3. What is an estimate? 4. What is point estimation? 5. What is interval estimation? 6. What is confidence interval? 7. What is null hypothesis? Give an example. 8. Define alternative hypothesis. 9. D...\n",
      "FAISS Retrieved Indices: [[ 4973  4974 17079  6128 17966]]\n",
      "FAISS Retrieved Distances: [[17.009455 20.02095  21.745953 22.084837 23.21171 ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "file_path = \"all_chunks.pkl\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "bm25 = BM25Okapi([chunk.split() for chunk in all_chunks])\n",
    "\n",
    "query = \"What is mass?\"\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "distances, indices = index.search(query_embedding, 5)\n",
    "retrieved_chunks = [all_chunks[i] for i in indices[0] if i < len(all_chunks)]\n",
    "\n",
    "bm25_scores = bm25.get_scores(query.split())\n",
    "bm25_indices = np.argsort(bm25_scores)[::-1][:5]\n",
    "bm25_chunks = [all_chunks[i] for i in bm25_indices]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFAISS Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(f\"- {chunk[:200]}...\")\n",
    "\n",
    "print(\"\\nBM25 Retrieved Chunks:\")\n",
    "for chunk in bm25_chunks:\n",
    "    print(f\"- {chunk[:200]}...\")\n",
    "    \n",
    "print(\"FAISS Retrieved Indices:\", indices)\n",
    "print(\"FAISS Retrieved Distances:\", distances)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
